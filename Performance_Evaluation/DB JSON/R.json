{"_id":{"$oid":"5c6d1ec099967322d0287f78"},"algorithm":"svm (e1071)","data":{"Argument":["formula","data","x","y","scale","type","kernel","degree","gamma","coef0","cost","nu","class.weights","cachesize","tolerance","epsilon","shrinking","cross","fitted","probability","…","subset","na.action"],"Description":["a symbolic description of the model to be fit.","an optional data frame containing the variables in the model.           By default the variables are taken from the environment which           ‘svm’ is called from.","a data matrix, a vector, or a sparse matrix (object of class     Matrix provided by the Matrix package,     or of class matrix.csr     provided by the SparseM package, or of class     simple_triplet_matrix provided by the slam     package).","a response vector with one label for each row/component of     x. Can be either a factor (for classification tasks)     or a numeric vector (for regression).","A logical vector indicating the variables to be     scaled. If scale is of length 1, the value is recycled as     many times as needed.     Per default, data are scaled internally (both x and y     variables) to zero mean and unit variance. The center and scale     values are returned and used for later predictions.","svm can be used as a classification     machine, as a regression machine, or for novelty detection.     Depending of whether y is     a factor or not, the default setting for type is C-classification or eps-regression, respectively, but may be overwritten by setting an explicit value.     Valid options are: C-classification nu-classification one-classification (for novelty detection) eps-regression nu-regression ","the kernel used in training and predicting. You     might consider changing some of the following parameters, depending     on the kernel type. linear:\\(u'v\\) polynomial:\\((\\gamma u'v + coef0)^{degree}\\) radial basis:\\(e^(-\\gamma |u-v|^2)\\) sigmoid:\\(tanh(\\gamma u'v + coef0)\\) ","parameter needed for kernel of type polynomial (default: 3)","parameter needed for all kernels except linear     (default: 1/(data dimension))","parameter needed for kernels of type polynomial     and sigmoid (default: 0)","cost of constraints violation (default: 1)---it is the     ‘C’-constant of the regularization term in the Lagrange formulation.","parameter needed for nu-classification,     nu-regression, and one-classification","a named vector of weights for the different     classes, used for asymmetric class sizes. Not all factor levels have     to be supplied (default weight: 1). All components have to be     named. Specifying \"inverse\" will choose the weights inversely     proportional to the class distribution.","cache memory in MB (default 40)","tolerance of termination criterion (default: 0.001)","epsilon in the insensitive-loss function (default: 0.1)","option whether to use the shrinking-heuristics     (default: TRUE)","if a integer value k>0 is specified, a k-fold cross     validation on the training data is performed to assess the quality     of the model: the accuracy rate for classification and the Mean     Squared Error for regression","logical indicating whether the fitted values should be computed     and included in the model or not (default: TRUE)","logical indicating whether the model should     allow for probability predictions.","additional parameters for the low level fitting function     svm.default","An index vector specifying the cases to be used in the           training sample.  (NOTE: If given, this argument must be           named.)","A function to specify the action to be taken if NAs are           found. The default action is na.omit, which leads to rejection of cases           with missing values on any required variable. An alternative   is na.fail, which causes an error if NA cases   are found. (NOTE: If given, this argument must be named.)"],"Default_value":[null,null,null,null,true,null,"radial",{"$numberInt":"3"},"if(is.vector(x))1else1/ncol(x)",{"$numberInt":"0"},{"$numberInt":"1"},{"$numberDouble":"0.5"},null,{"$numberInt":"40"},{"$numberDouble":"0.001"},{"$numberDouble":"0.1"},true,{"$numberInt":"0"},true,false,null,null,"na.omit"]}}
{"_id":{"$oid":"5c6d1ed599967322d0287f7a"},"algorithm":"knn (kknn)","data":{"Argument":["formula","train","test","learn","valid","na.action","k","distance","kernel","ykernel","scale","contrasts"],"Description":["A formula object.","Matrix or data frame of training set cases.","Matrix or data frame of test set cases.","Matrix or data frame of training set cases.","Matrix or data frame of test set cases.","A function which indicates what should happen when the data contain 'NA's.","Number of neighbors considered.","Parameter of Minkowski distance.","Kernel to use. Possible choices are \"rectangular\" (which is standard unweighted knn), \"triangular\", \"epanechnikov\" (or beta(2,2)),  \"biweight\" (or beta(3,3)), \"triweight\" (or beta(4,4)), \"cos\", \"inv\", \"gaussian\", \"rank\" and \"optimal\".","Window width of an y-kernel, especially for prediction of ordinal classes.","logical, scale variable to have equal sd.","A vector containing the 'unordered' and 'ordered' contrasts to use."],"Default_value":["formula(train)",null,null,null,null,"na.omit()",{"$numberInt":"10"},{"$numberInt":"2"},"optimal",null,true,"c('unordered'=contr.dummyordered=contr.ordinal)"]}}
{"_id":{"$oid":"5c6d1f0499967322d0287f7c"},"algorithm":"ann (neuralnet)","data":{"Argument":["formula","data","hidden","threshold","stepmax","rep","startweights","learningrate.limit","learningrate.factor","learningrate","lifesign","lifesign.step","algorithm","err.fct","act.fct","linear.output","exclude","constant.weights","likelihood"],"Description":["a symbolic description of the model to be fitted.","a data frame containing the variables specified in formula.","a vector of integers specifying the number of hidden neurons (vertices) in each layer.","a numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria.","the maximum steps for the training of the neural network. Reaching this maximum leads to a stop of the neural network's training process.","the number of repetitions for the neural network's training.","a vector containing starting values for the weights. The weights will not be randomly initialized.","a vector or a list containing the lowest and highest limit for the learning rate. Used only for RPROP and GRPROP.","a vector or a list containing the multiplication factors for the upper and lower learning rate. Used only for RPROP and GRPROP.","a numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation.","a string specifying how much the function will print during the calculation of the neural network. 'none', 'minimal' or 'full'.","an integer specifying the stepsize to print the minimal threshold in full lifesign mode.","a string containing the algorithm type to calculate the neural network. The  following types are possible: 'backprop', 'rprop+', 'rprop-', 'sag', or 'slr'. 'backprop' refers to backpropagation, 'rprop+' and 'rprop-' refer to  the resilient backpropagation with and without weight backtracking,  while 'sag' and 'slr' induce the usage of the modified globally convergent algorithm (grprop). See Details for more information.","a differentiable function that is used for the calculation of the error. Alternatively, the strings 'sse' and 'ce' which stand for the sum of squared errors and the cross-entropy can be used.","a differentiable function that is used for smoothing the result of the cross product of the covariate or neurons and the weights. Additionally the strings, 'logistic' and 'tanh' are possible for the logistic function and tangent hyperbolicus.","logical. If act.fct should not be applied to the output neurons set linear output to TRUE, otherwise to FALSE.","a vector or a matrix specifying the weights, that are excluded from the calculation. If given as a vector, the exact positions of the weights must be known. A matrix with n-rows and 3 columns will exclude n weights, where the first column stands for the layer, the second column for the input neuron and the third column for the output neuron of the weight.","a vector specifying the values of the weights that are excluded from the training process and treated as fix.","logical. If the error function is equal to the negative log-likelihood function, the information criteria AIC and BIC will be calculated. Furthermore the usage of confidence.interval is meaningfull."],"Default_value":[null,null,{"$numberInt":"1"},{"$numberDouble":"0.01"},{"$numberInt":"100000"},{"$numberInt":"1"},null,null,"list(minus=0.5plus=1.2)",null,null,{"$numberInt":"1000"},"rprop+","sse","logistic",true,null,null,false]}}
{"_id":{"$oid":"5c6d1f1899967322d0287f7e"},"algorithm":"decision tree (tree)","data":{"Argument":["formula","data","weights","subset","na.action","control","method","split","model","x","y","wts","…"],"Description":["A formula expression. The left-hand-side (response)     should be either a numerical vector when a regression tree will be     fitted or a factor, when a classification tree is produced. The     right-hand-side should be a series of numeric or factor     variables separated by +; there should be no interaction     terms. Both . and - are allowed: regression trees can     have offset terms.","A data frame in which to preferentially interpret     formula, weights and subset.","Vector of non-negative observational weights; fractional     weights are allowed.","An expression specifying the subset of cases to be used.","A function to filter missing data from the model     frame. The default is na.pass (to do nothing) as tree     handles missing values (by dropping them down the tree as far     as possible).","A list as returned by tree.control.","character string giving the method to use. The only other     useful value is \"model.frame\".","Splitting criterion to use.","If this argument is itself a model frame, then the     formula and data arguments are ignored, and     model is used to define the model.  If the argument is     logical and true, the model frame is stored as component     model in the result.","logical. If true, the matrix of variables for each case     is returned.","logical. If true, the response variable is returned.","logical. If true, the weights are returned.","Additional arguments that are passed to     tree.control. Normally used for mincut, minsize     or mindev."],"Default_value":[null,null,null,null,"na.pass","tree.control(nobs...)","recursive.partition","c(deviancegini)",false,false,true,true,null]}}
{"_id":{"$oid":"5c6d1f8899967322d0287f86"},"algorithm":"svm (MachineShop)","data":{"Argument":["scaled","type","kernel","kpar","C","nu","epsilon","cache","tol","shrinking","sigma","degree","order","scale","offset"],"Description":["logical vector indicating the variables to be scaled.","type of support vector machine.","kernel function used in training and predicting.","list of hyper-parameters (kernel parameters).","cost of constraints violation defined as the regularization term in the Lagrange formulation.","parameter needed for nu-svc, one-svc, and nu-svr.","parameter in the insensitive-loss function used for eps-svr, nu-svr and eps-bsvm.","cache memory in MB.","tolerance of termination criterion.","whether to use the shrinking-heuristics.","inverse kernel width used by the ANOVA, Bessel, and Laplacian kernels.","degree of the ANOVA, Bessel, and polynomial kernel functions.","order of the Bessel function to be used as a kernel.","scaling parameter of the polynomial and hyperbolic tangent kernels as a convenient way of normalizing patterns without the need to modify the data itself.","offset used in polynomial and hyperbolic tangent kernels."],"Default_value":[true,null,"c(rbfdotpolydotvanilladottanhdotlaplacedotbesseldotanovadotsplinedot)","automatic",{"$numberInt":"1"},{"$numberDouble":"0.2"},{"$numberDouble":"0.1"},{"$numberInt":"40"},{"$numberDouble":"0.001"},true,null,{"$numberInt":"1"},{"$numberInt":"1"},{"$numberInt":"1"},{"$numberInt":"1"}]}}
