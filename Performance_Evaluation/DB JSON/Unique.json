{"_id":{"$oid":"5c211047dbfdab4ad0925844"},"Algo":"SVM","Parameters":["kernel","cost","gamma","tolerance","cache_size","degree","coef0","shrinking","probability","class_weight"],"Description":["Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples). ","cost of constraints violation (default: 1)---it is the     ‘C’-constant of the regularization term in the Lagrange formulation.","parameter needed for all kernels except linear     (default: 1/(data dimension))","Tolerance for stopping criterion.","Specify the size of the kernel cache (in MB). ","parameter needed for kernel of type polynomial (default: 3)","Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’. ","Whether to use the shrinking heuristic. ","logical indicating whether the model should     allow for probability predictions.","a named vector of weights for the different     classes, used for asymmetric class sizes. Not all factor levels have     to be supplied (default weight: 1). All components have to be     named. Specifying \"inverse\" will choose the weights inversely     proportional to the class distribution."]}
{"_id":{"$oid":"5c219946dbfdab4ad0925845"},"Algo":"KNN","Parameters":["k"],"Description":["number of neighbours considered"]}
{"_id":{"$oid":"5c3b5e4a05f03058bc517256"},"Algo":"Decision Tree","Parameters":["Split","weight"],"Description":["The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split","Vector of non-negative observational weights; fractional weights are allowed"]}
{"_id":{"$oid":"5c40a0b5b94f2b431898e539"},"Algo":"ANN","Parameters":["hidden_layer_sizes","activation","learningrate"],"Description":["The ith element represents the number of neurons in the ith hidden layer","a differentiable function that is used for smoothing the result of the cross product of the covariate or neurons and the weights. Additionally the strings, 'logistic' and 'tanh' are possible for the logistic function and tangent hyperbolicus","a numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation"]}
