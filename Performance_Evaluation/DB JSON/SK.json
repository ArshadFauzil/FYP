{"_id":{"$oid":"5c6d1cfc99967322d0287f6e"},"algorithm":"svm (sklearn)","data":{"Argument":["C","kernel","degree","gamma","coef0","shrinking","probability","tol","cache_size","class_weight","verbose","max_iter","decision_function_shape","random_state"],"Description":["Penalty parameter C of the error term. ","Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples). ","Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels. ","Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Current default is ‘auto’ which uses 1 / n_features, if gamma='scale' is passed then it uses 1 / (n_features * X.std()) as value of gamma. The current default of gamma, ‘auto’, will change to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of ‘auto’ is used as a default indicating that no explicit value of gamma was passed. ","Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’. ","Whether to use the shrinking heuristic. ","Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method. ","Tolerance for stopping criterion. ","Specify the size of the kernel cache (in MB). ","Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) ","Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ","Hard limit on iterations within solver, or -1 for no limit. ","Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one (‘ovo’) is always used as multi-class strategy.  Changed in version 0.19: decision_function_shape is ‘ovr’ by default.   New in version 0.17: decision_function_shape=’ovr’ is recommended.   Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None.  ","The seed of the pseudo random number generator used when shuffling the data for probability estimates. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. "],"Default_value":[{"$numberInt":"1"},"rbf",{"$numberInt":"3"},"auto",{"$numberInt":"0"},true,false,{"$numberDouble":"0.001"},null,null,false,{"$numberInt":"-1"},"ovr",null]}}
{"_id":{"$oid":"5c6d1d1d99967322d0287f70"},"algorithm":"knn (sklearn)","data":{"Argument":["n_neighbors","weights","algorithm","leaf_size","p","metric","metric_params","n_jobs"],"Description":["Number of neighbors to use by default for kneighbors queries. ","weight function used in prediction.  Possible values:  ‘uniform’ : uniform weights.  All points in each neighborhood are weighted equally. ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  ","Algorithm used to compute the nearest neighbors:  ‘ball_tree’ will use BallTree ‘kd_tree’ will use KDTree ‘brute’ will use a brute-force search. ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ","Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. ","Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ","the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. ","Additional keyword arguments for the metric function. ","The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. Doesn’t affect fit method. "],"Default_value":[{"$numberInt":"5"},"uniform",null,{"$numberInt":"30"},{"$numberInt":"2"},"minkowski",null,null]}}
{"_id":{"$oid":"5c6d1e0499967322d0287f74"},"algorithm":"ann MLPClassifier (sklearn)","data":{"Argument":["hidden_layer_sizes","activation","solver","alpha","batch_size","learning_rate","learning_rate_init","power_t","max_iter","shuffle","random_state","tol","verbose","warm_start","momentum","nesterovs_momentum","early_stopping","validation_fraction","beta_1","beta_2","epsilon","n_iter_no_change"],"Description":["The ith element represents the number of neurons in the ith hidden layer. ","Activation function for the hidden layer.  ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x). ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)  ","The solver for weight optimization.  ‘lbfgs’ is an optimizer in the family of quasi-Newton methods. ‘sgd’ refers to stochastic gradient descent. ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba  Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better. ","L2 penalty (regularization term) parameter. ","Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples) ","Learning rate schedule for weight updates.  ‘constant’ is a constant learning rate given by ‘learning_rate_init’. ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t) ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5.  Only used when solver='sgd'. ","The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’. ","The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’. ","Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps. ","Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’. ","If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. ","Tolerance for the optimization. When the loss or score is not improving by at least tol for n_iter_no_change consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops. ","Whether to print progress messages to stdout. ","When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary. ","Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’. ","Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0. ","Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. Only effective when solver=’sgd’ or ‘adam’ ","The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True ","Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=’adam’ ","Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=’adam’ ","Value for numerical stability in adam. Only used when solver=’adam’ ","Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’  New in version 0.20.  "],"Default_value":[{"$numberInt":"100"},"relu","adam",{"$numberDouble":"0.0001"},"auto","constant",{"$numberDouble":"0.001"},{"$numberDouble":"0.5"},{"$numberInt":"200"},true,null,{"$numberDouble":"0.0001"},false,false,{"$numberDouble":"0.9"},true,false,{"$numberDouble":"0.1"},{"$numberDouble":"0.9"},{"$numberDouble":"0.999"},{"$numberDouble":"1e-8"},{"$numberInt":"10"}]}}
{"_id":{"$oid":"5c6d1e2a99967322d0287f76"},"algorithm":"decision tree (sklearn)","data":{"Argument":["criterion","splitter","max_depth","min_samples_split","min_samples_leaf","min_weight_fraction_leaf","max_features","random_state","max_leaf_nodes","min_impurity_decrease","min_impurity_split","class_weight","presort"],"Description":["The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. ","The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split. ","The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ","The minimum number of samples required to split an internal node:  If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.   Changed in version 0.18: Added float values for fractions.  ","The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.  If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.   Changed in version 0.18: Added float values for fractions.  ","The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ","The number of features to consider when looking for the best split:   If int, then consider max_features features at each split. If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.   Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features. ","If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. ","Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ","A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: N_t / N * (impurity - N_t_R / N_t * right_impurity                     - N_t_L / N_t * left_impurity)   where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child. N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.  New in version 0.19.  ","Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead.  ","Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ","Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training. "],"Default_value":["”gini”","”best”",null,{"$numberInt":"2"},{"$numberInt":"1"},{"$numberInt":"0"},null,null,null,{"$numberInt":"0"},{"$numberDouble":"1e-7"},null,false]}}
